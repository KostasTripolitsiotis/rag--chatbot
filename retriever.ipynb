{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "28c1028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from rank_bm25 import BM25Okapi\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95714d9a",
   "metadata": {},
   "source": [
    "### PDF to List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ebc0a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_extracted_text_from_data():\n",
    "    \"\"\"\n",
    "    Extract text from all PDF files in /data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the text from the provided PDF files\n",
    "    pdf2 = \"data/pdf_2.pdf\"\n",
    "    doc2 = fitz.open(pdf2)\n",
    "    pdf4 = \"data/pdf_4.pdf\"\n",
    "    doc4 = fitz.open(pdf4)\n",
    "    pdf5 = \"data/pdf_5.pdf\"\n",
    "    doc5 = fitz.open(pdf5)\n",
    "    pdf6 = \"data/pdf_6.pdf\"\n",
    "    doc6 = fitz.open(pdf6)\n",
    "    pdf7 = \"data/pdf_7.pdf\"\n",
    "    doc7 = fitz.open(pdf7)\n",
    "\n",
    "    extracted_text = \"\"\n",
    "\n",
    "    # Combine the text from all pages of the PDFs\n",
    "    for doc in [doc2, doc4, doc5, doc6, doc7]:\n",
    "        for page_num in range(doc.page_count):\n",
    "            page = doc[page_num]\n",
    "            extracted_text += page.get_text()\n",
    "\n",
    "    doc2.close()\n",
    "    doc4.close()\n",
    "    doc5.close()\n",
    "    doc6.close()\n",
    "    doc7.close()\n",
    "    \n",
    "    return extracted_text\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e9aa72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: 23037\n"
     ]
    }
   ],
   "source": [
    "# # Check the length of the extracted text\n",
    "# extracted_text = get_extracted_text_from_data()\n",
    "\n",
    "# word_count = len(extracted_text.split())\n",
    "# print(f\"Total words: {word_count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f874f1",
   "metadata": {},
   "source": [
    "### Chunking & Database Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c97d4a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chunks_from_text(extracted_text):\n",
    "    \"\"\"\n",
    "    Split the extracted text into chunks.\n",
    "    \"\"\"\n",
    "    # Initialize the text splitter with desired chunk size and overlap\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500, \n",
    "        chunk_overlap=50)\n",
    "    \n",
    "    # Split the text into chunks\n",
    "    chunks = text_splitter.split_text(extracted_text)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e090df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks: 340\n"
     ]
    }
   ],
   "source": [
    "# # Check the number of chunks created\n",
    "# chunks = get_chunks_from_text(extracted_text)\n",
    "# print(f\"Number of chunks: {len(chunks)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c95262a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Save the chunks to a csv file\n",
    "def save_chunks_to_csv(chunks):\n",
    "    \"\"\"\n",
    "    Save the chunks to a file.\n",
    "    \"\"\"\n",
    "    # Create the dataframe with the chunks\n",
    "    df = pd.DataFrame(chunks, columns=[\"text\"])\n",
    "    # Save the chunks to a csv dafaframe\n",
    "    df.to_csv(\"data/chunks.csv\")\n",
    "    \n",
    "# Method 2: Get the chunks from the csv file\n",
    "def get_chunks_from_csv():\n",
    "    \"\"\"\n",
    "    Get the chunks from the csv file.\n",
    "    \"\"\"\n",
    "    # Read the csv file\n",
    "    df = pd.read_csv(\"data/chunks.csv\")\n",
    "    # Get the chunks from the dataframe\n",
    "    chunks = df[\"text\"].tolist()\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0688f7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_chunks_to_csv(chunks)\n",
    "# chunks = get_chunks_from_csv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ea2f44",
   "metadata": {},
   "source": [
    "### Transform chunks to vectors for FAISS retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "140f1489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model function\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Load the SentenceTransformer model.\n",
    "    \"\"\"\n",
    "    # Load the SentenceTransformer model\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Embedding each chunk of text\n",
    "def embed_chunks(chunks, model):\n",
    "    \"\"\"\n",
    "    Embed the chunks of text.\n",
    "    \"\"\"\n",
    "    # Encode the chunks\n",
    "    embeddings = model.encode(chunks)\n",
    "    \n",
    "    return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fccd6a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create faiss index and store the embeddings\n",
    "def create_faiss_index(embeddings):\n",
    "    \"\"\"\n",
    "    Create faiss index and store the embeddings\n",
    "    \"\"\"\n",
    "\n",
    "    # Store in FAISS index\n",
    "    dim = len(embeddings[0]) # Dimension of the embedding\n",
    "    index = faiss.IndexFlatL2(dim) # L2 distance index\n",
    "    \n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e624d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the model\n",
    "# model = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ff800f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "340\n"
     ]
    }
   ],
   "source": [
    "# embeddings = embed_chunks(chunks, model) # Embed the chunks\n",
    "# index = create_faiss_index(embeddings) # Create the faiss index\n",
    "\n",
    "# print(index.is_trained) # Check if the index is trained\n",
    "# index.add(np.array(embeddings)) # Add the embeddings to the index\n",
    "# print(index.ntotal) # Number of vectors in the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056bb96f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Save the index to a file\n",
    "# faiss.write_index(index, \"data/faiss_index.index\") # Save the index to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a92199ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the faiss index from a file   \n",
    "def load_faiss_index():\n",
    "    \"\"\"\n",
    "    Load the faiss index from a file.\n",
    "    \"\"\"\n",
    "    index = faiss.read_index(\"data/faiss_index.index\") # Load the index from a file\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e767d7b8",
   "metadata": {},
   "source": [
    "### Retrieving form FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e56b7c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top k most similar chunks from the faiss index using a query\n",
    "def get_top_k_faiss(index, model, query, k):\n",
    "    \"\"\"\n",
    "    Get the top k most similar chunks from the faiss index.\n",
    "    \"\"\"\n",
    "    # Search for the top k most similar chunks\n",
    "    results = index.search(model.encode([query]), k=k)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ec69c5",
   "metadata": {},
   "source": [
    "##### Chech results from FAISS retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a41bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Declare given queries\n",
    "# query1 = \"What features does MATLAB offer to help shorten response times and reduce data transmission over the network?\"\n",
    "# query2 = \"How did Baker Hughes engineers use MATLAB to develop pump health monitoring software?\"\n",
    "# query3 = \"Why is it important for training data in predictive maintenance systems to include instances from both normal and fault conditions?\"\n",
    "# query4 = \"What is the recall performance of the proposed ENBANN method in comparison to other methods?\"\n",
    "# query5 = \"What is cross-sectional prediction and how can it be applied in estimating component lifespan?\"\n",
    "# query6 = \"Why are gas leak detectors important in environments with many pneumatic valves, and what type of detectors are considered non-intrusive?\"\n",
    "# query7 = \"What new Industry 4.0 technologies are being used for remote asset monitoring, and what tools support them?\"\n",
    "# query8 = \"What does the simulation model of the SUDM policy evaluate, and what assumptions are made about workstation operations?\"\n",
    "# query9 = \"How were the prior parameters for the Weibull and exponential degradation models estimated, and what assumptions were made about the error terms?\"\n",
    "# query10 = \"How does fuzzy logic contribute to diagnostics in machine failure and maintenance management?\"\n",
    "# query11 = \"Why are artificial neural networks suitable for prognostics in machine failure, and what limitations do traditional systems face?\"\n",
    "# query12 = \"How do Big Data platforms and CMMS contribute to the formulation of maintenance strategies?\"\n",
    "# query13 = \"What is the relationship between diagnostics and prognostics in the context of machine degradation and failure?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2dac4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([[0.97686744, 1.0720754 , 1.0859646 , 1.110936  , 1.130651  ]],\n",
      "      dtype=float32), array([[ 8,  4,  5,  3, 14]], dtype=int64))\n",
      "[8, 4, 5, 3, 14]\n"
     ]
    }
   ],
   "source": [
    "# # Check results for query1\n",
    "# results = get_top_k_faiss(index, model, query1, k=5) # Get the top 5 most similar chunks\n",
    "\n",
    "# print(results) # Print the results\n",
    "# # Print the chuncks of the 5 most similar chunks\n",
    "\n",
    "# print((results[1][0].tolist())) # Print the indices of the most similar chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee0339e",
   "metadata": {},
   "source": [
    "### Retreaving with BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d6deda52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get BM25 retriever\n",
    "def get_bm25_retriever(chunks):\n",
    "    \"\"\"\n",
    "    Get BM25 retriever.\n",
    "    \"\"\"\n",
    "    # Tokenize the chunks\n",
    "    tokenized_corpus = [doc.split(\" \") for doc in chunks]\n",
    "    \n",
    "    # Create the BM25 retriever\n",
    "    retriever = BM25Okapi(tokenized_corpus)\n",
    "    \n",
    "    return retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "af0f4dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get results with BM25 method\n",
    "def get_top_k_bm25(retriever, query, chunks, k):\n",
    "    \"\"\"\n",
    "    Get the top k most similar chunks from the BM25 retriever.\n",
    "    \"\"\"\n",
    "    # Tokenize the query\n",
    "    tokenized_query = query.split(\" \")\n",
    "    \n",
    "    # Get the top k most similar chunks\n",
    "    results = retriever.get_top_n(tokenized_query, chunks, n=k)\n",
    "    \n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ee994f",
   "metadata": {},
   "source": [
    "##### Chech results from BM25 retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d33e1c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retriever = get_bm25_retriever(chunks) # Get the BM25 retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a007410",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read user story \n",
      "Develop Predictive Models\n",
      "Interactively train and evaluate predictive models using the \n",
      "Classification Learner app.\n",
      "6\n",
      "Predictive Maintenance with MATLAB\n",
      "Once you’ve developed your models, you want to get them up \n",
      "and running as quickly as possible.  MATLAB integrates into \n",
      "enterprise systems, clusters, and clouds, and can be targeted to \n",
      "real-time embedded hardware.\n",
      "To shorten response times and send less data over the network, \n",
      "you can deploy the models directly on machines.\n",
      "[8, 159, 106, 6, 20]\n"
     ]
    }
   ],
   "source": [
    "# results_bm = get_top_k_bm25(retriever, query1, k=5) # Get the top 5 most similar chunks\n",
    "\n",
    "# # Print the results\n",
    "# print(results_bm[0])\n",
    "\n",
    "# index_bm = [chunks.index(result) for result in results_bm]\n",
    "\n",
    "# print(index_bm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58aacd63",
   "metadata": {},
   "source": [
    "### Compare 2 results and get final retrieved chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "82b02ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_5_chunks(chunks, query, index, model, retriever, k):\n",
    "    \"\"\"\n",
    "    Get the top k most similar chunks from the chunks.\n",
    "    \"\"\"\n",
    "    \n",
    "    results_faiss = get_top_k_faiss(index, model, query, k*2) # Get the top k most similar chunks from the faiss index\n",
    "    index_faiss = results_faiss[1][0].tolist() # Get the indexes of the chunks from the faiss retriever\n",
    "    \n",
    "    results_bm = get_top_k_bm25(retriever, query, chunks, k*2) # Get the top k most similar chunks from the BM25 retriever\n",
    "    index_bm = [chunks.index(result) for result in results_bm] # Get the indexes of the chunks from the BM25 retriever\n",
    "    \n",
    "    indexes = list(filter(lambda x: x in index_faiss, index_bm)) # Get the common indexes from both retrievers\n",
    "    \n",
    "    # Handle the case when there are less than 5 common indexes\n",
    "    if len(indexes) < 5:\n",
    "        return get_top_5_chunks(chunks, query, index, model, retriever, k+5) # If there are less than 5 common indexes, get more chunks\n",
    "    \n",
    "    indexes = indexes[:5] # Get the top 5 indexes\n",
    "    \n",
    "    chunks = [chunks[i] for i in indexes] # Get the chunks from the indexes\n",
    "    \n",
    "    return chunks, indexes # Return the chunks and indexes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e879341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks = get_chunks_from_csv() # Get the chunks from the csv file\n",
    "\n",
    "# chunks_res, index_res = get_top_5_chunks(chunks, query1, k=5) # Get the top 5 most similar chunks from the chunks\n",
    "\n",
    "# print(index_res) # Print the indexes of the most similar chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd35f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes: [12, 11, 10, 14, 13]\n",
      "Top 5 chunks:\n",
      "Chunk 1: needed to determine when a pump was about to fail. Too-frequent \n",
      "maintenance wasted effort and resulted in still-usable parts being \n",
      "replaced, while too-infrequent maintenance risked damaging \n",
      "pumps beyond repair.\n",
      "Working in MATLAB, Baker Hughes engineers developed \n",
      "pump health monitoring software that applies machine learning \n",
      "techniques in real time to predict the ideal time to perform \n",
      "maintenance. They processed and analyzed up to a terabyte\n",
      "Chunk 2: 7\n",
      "Predictive Maintenance with MATLAB\n",
      "Industry Example\n",
      "Truck with positive displacement pump.\n",
      "Baker Hughes Develops Predictive Maintenance Software for Gas \n",
      "and Oil Extraction Equipment\n",
      "Baker Hughes trucks are equipped with positive displacement \n",
      "pumps that inject a mixture of water and sand at high pressures \n",
      "deep into drilled wells. With pumps accounting for about \n",
      "$100,000 of the $1.5 million total cost of the truck, Baker Hughes\n",
      "Chunk 3: machines nonstop, even on Christmas, and we rely on our MATLAB based monitoring and \n",
      "predictive maintenance software to run continuously and reliably in production.”\n",
      "— Dr. Michael Kohlert, Mondi\n",
      " Read user story\n",
      "Deploy Models in Production\n",
      "• Engineering, scientiﬁc, and ﬁeld\n",
      "• Business and transactional\n",
      "DATA\n",
      "ENTERPRISE IT SYSTEMS\n",
      "EMBEDDED SYSTEMS\n",
      "Deploy MATLAB analytics to run in \n",
      "embedded systems \n",
      "Deploy MATLAB analytics\n",
      "to enterprise IT systems\n",
      "7\n",
      "Predictive Maintenance with MATLAB\n",
      "Chunk 4: using lower-level language libraries for all the built-in MATLAB \n",
      "capabilities we needed, it would likely have taken an order of \n",
      "magnitude longer to complete this project.”\n",
      "– Gulshan Singh, Baker Hughes\n",
      " Read user story\n",
      "Explore these resources to learn more about developing and deploying \n",
      "predictive maintenance models with MATLAB.\n",
      "Watch\n",
      "Predictive Maintenance with MATLAB: A Prognostics Case Study 52:22\n",
      "Signal Processing and Machine Learning Techniques for Sensor Data Analytics 42:45\n",
      "Read\n",
      "Chunk 5: of data collected at 50,000 samples per second from sensors \n",
      "installed on 10 trucks operating in the field, identified the \n",
      "parameters that were useful in predicting failures, and created \n",
      "and trained a neural network to use sensor data to predict pump \n",
      "failures.\n",
      "The software is expected to reduce maintenance costs by \n",
      "30–40%—or more than $10 million.\n",
      "“MATLAB enabled us to perform our analyses and processing, \n",
      "including machine learning.  . . . If we had to write our own code\n"
     ]
    }
   ],
   "source": [
    "# def __main__():\n",
    "#     # Get the chunks from the csv file\n",
    "#     chunks = get_chunks_from_csv()\n",
    "    \n",
    "#     # Load the model\n",
    "#     model = load_model()\n",
    "    \n",
    "#     # Load faiss index\n",
    "#     index = load_faiss_index()\n",
    "    \n",
    "#     # Get BM25 retriever\n",
    "#     retriever = get_bm25_retriever(chunks)\n",
    "    \n",
    "#     chunks_res, chunks_index = get_top_5_chunks(chunks, query2, index, model, retriever, k=5) # Get the top 5 most similar chunks from the chunks\n",
    "    \n",
    "#     # Print the results\n",
    "#     print(f\"Indexes: {chunks_index}\") # Print the indexes of the most similar chunks\n",
    "#     print(\"Top 5 chunks:\")\n",
    "#     for i, chunk in enumerate(chunks_res):\n",
    "#         print(f\"Chunk {i+1}: {chunk}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "# if __name__ == \"__main__\":\n",
    "#     __main__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22d8cd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chunk 1\\nneeded to determine when a pump was about to fail. Too-frequent \\nmaintenance wasted effort and resulted in still-usable parts being \\nreplaced, while too-infrequent maintenance risked damaging \\npumps beyond repair.\\nWorking in MATLAB, Baker Hughes engineers developed \\npump health monitoring software that applies machine learning \\ntechniques in real time to predict the ideal time to perform \\nmaintenance. They processed and analyzed up to a terabyte\\n\\nChunk 2\\n7\\nPredictive Maintenance with MATLAB\\nIndustry Example\\nTruck with positive displacement pump.\\nBaker Hughes Develops Predictive Maintenance Software for Gas \\nand Oil Extraction Equipment\\nBaker Hughes trucks are equipped with positive displacement \\npumps that inject a mixture of water and sand at high pressures \\ndeep into drilled wells. With pumps accounting for about \\n$100,000 of the $1.5 million total cost of the truck, Baker Hughes\\n\\nChunk 3\\nmachines nonstop, even on Christmas, and we rely on our MATLAB based monitoring and \\npredictive maintenance software to run continuously and reliably in production.”\\n— Dr. Michael Kohlert, Mondi\\n Read user story\\nDeploy Models in Production\\n• Engineering, scientiﬁc, and ﬁeld\\n• Business and transactional\\nDATA\\nENTERPRISE IT SYSTEMS\\nEMBEDDED SYSTEMS\\nDeploy MATLAB analytics to run in \\nembedded systems \\nDeploy MATLAB analytics\\nto enterprise IT systems\\n7\\nPredictive Maintenance with MATLAB\\n\\nChunk 4\\nusing lower-level language libraries for all the built-in MATLAB \\ncapabilities we needed, it would likely have taken an order of \\nmagnitude longer to complete this project.”\\n– Gulshan Singh, Baker Hughes\\n Read user story\\nExplore these resources to learn more about developing and deploying \\npredictive maintenance models with MATLAB.\\nWatch\\nPredictive Maintenance with MATLAB: A Prognostics Case Study 52:22\\nSignal Processing and Machine Learning Techniques for Sensor Data Analytics 42:45\\nRead\\n\\nChunk 5\\nof data collected at 50,000 samples per second from sensors \\ninstalled on 10 trucks operating in the field, identified the \\nparameters that were useful in predicting failures, and created \\nand trained a neural network to use sensor data to predict pump \\nfailures.\\nThe software is expected to reduce maintenance costs by \\n30–40%—or more than $10 million.\\n“MATLAB enabled us to perform our analyses and processing, \\nincluding machine learning.  . . . If we had to write our own code\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def ask(query):\n",
    "    \"\"\"\n",
    "    Ask a question and get the answer.\n",
    "    \"\"\"\n",
    "    # Get the chunks from the csv file\n",
    "    chunks = get_chunks_from_csv()\n",
    "    \n",
    "    # Load the model\n",
    "    model = load_model()\n",
    "    \n",
    "    # Load faiss index\n",
    "    index = load_faiss_index()\n",
    "    \n",
    "    # Get BM25 retriever\n",
    "    retriever = get_bm25_retriever(chunks)\n",
    "    \n",
    "    chunks_res, chunks_index = get_top_5_chunks(chunks, query, index, model, retriever, k=5) # Get the top 5 most similar chunks from the chunks\n",
    "    \n",
    "    for i, chunk in enumerate(chunks_res):\n",
    "        chunks_res[i] = \"Chunk \" + str(i+1) + \":\\n\" + chunk + \"\\n\"# Add the chunk number to the chunk\n",
    "    \n",
    "    # chunks_res = \"\\n\".join(chunks_res) # Join the chunks to get the final result\n",
    "    \n",
    "    return chunks_res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
